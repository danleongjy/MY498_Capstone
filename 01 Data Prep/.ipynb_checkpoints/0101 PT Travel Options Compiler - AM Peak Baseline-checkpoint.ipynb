{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import time as tm\n",
    "import sqlite3\n",
    "import traceback\n",
    "import sys\n",
    "from math import cos, asin, sqrt\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import os\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tfl_journeyplanner_api_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 983\n",
      "966289 pairs generated. First 10: [(('E02006854', '-0.025046237', '51.50164399'), ('E02000001', '-0.091155997', '51.51475914')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000002', '0.136596432', '51.58623085')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000003', '0.138810779', '51.5705831')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000004', '0.17624198', '51.56078086')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000005', '0.144375564', '51.56175313')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000007', '0.154249648', '51.55985824')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000008', '0.138580433', '51.55208708')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000009', '0.118662175', '51.55239182')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000010', '0.150976445', '51.5482683')), (('E02006854', '-0.025046237', '51.50164399'), ('E02000011', '0.162473217', '51.54805375'))]\n"
     ]
    }
   ],
   "source": [
    "# Read in origin longlats: pop-weighted MSOA centroids\n",
    "orig_coords = []\n",
    "with open('PopWeightedCentroids2011LondonMSOA_coords.csv', 'r') as inputfile:\n",
    "    reader = csv.reader(inputfile, delimiter = ',')\n",
    "    for row in reader:\n",
    "        orig_coords.append((row[0], row[4], row[5])) # read in longlats coords from csv file into tuples\n",
    "orig_coords = orig_coords[1:]\n",
    "\n",
    "# Read in destination longlats: jobs-weighted MSOA centroids\n",
    "dest_coords = []\n",
    "with open('JobsWeightedCentroids2011LondonMSOA_coords.csv', 'r') as inputfile:\n",
    "    reader = csv.reader(inputfile, delimiter = ',')\n",
    "    for row in reader:\n",
    "        dest_coords.append((row[0], row[1], row[2])) # read in longlats coords from csv file into tuples\n",
    "dest_coords = dest_coords[1:]\n",
    "\n",
    "odpairs = GenerateODPairs(orig_coords[:], dest_coords[:]) # specify how many OD pairs to generate here\n",
    "print(len(orig_coords), len(dest_coords)) # checking data import was successful\n",
    "print(len(odpairs), \"pairs generated. First 10:\", odpairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up db\n",
    "'''\n",
    "Already run once - do not rerun! It will reset the database!\n",
    "\n",
    "db = batchsettings['date'] + '_' + batchsettings['time'] + '/traveloptions_to_top10.db'\n",
    "if not os.path.exists(batchsettings['date'] + '_' + batchsettings['time']):\n",
    "    os.makedirs(batchsettings['date'] + '_' + batchsettings['time'])\n",
    "setup_db(db)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12838 0 0\n"
     ]
    }
   ],
   "source": [
    "# set up network data\n",
    "with open('stoplocations.json', 'r') as infile:\n",
    "    stoplocations = json.load(infile)\n",
    "stoplist = {}\n",
    "for stop in stoplocations['features']:\n",
    "    stoplist[stop['properties']['NAPTAN']] = stop['geometry']['coordinates']\n",
    "#with open('20190322_0830/networkdata_800000_900000', 'rb') as infile:\n",
    "#    edges, nodes = pickle.load(infile)\n",
    "edges = {}\n",
    "nodes = {}\n",
    "print(len(stoplist), len(edges), len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16289\n"
     ]
    }
   ],
   "source": [
    "# set up currentpairs\n",
    "# msoa_top10_jobs = ['E02000809','E02000970','E02000808','E02000575','E02000979',\n",
    "#                    'E02000193','E02006854','E02000972','E02000977','E02000001'] # 10 MSOAs containing the most jobs\n",
    "currentpairs = [odpair for odpair in odpairs[950000:]] # updated 20190218/0048\n",
    "#with open('20190322_0830/currentpairs850000', 'rb') as infile:\n",
    "#    currentpairs = pickle.load(infile)\n",
    "print(len(currentpairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up batchsettings\n",
    "batchsettings = {'processingstarttime': datetime.datetime(2019,2,17,20,0),\n",
    "                 'processingendtime': datetime.datetime(2019,2,18,6,0),\n",
    "                 'date': '&date=20190322',\n",
    "                 'time': '&time=0830',\n",
    "                 'timeIs': '&timeIs=Arriving',\n",
    "                 'journeyPreference': '&journeyPreference=leastTime',\n",
    "                 'useMultiModalCall': '&useMultiModalCall=true'}\n",
    "db = '20190322_0830/traveloptions.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing: 16289 pairs in 57 batches.\n",
      "Processed batch 0 in 46.93143362640694 s ( 79.0 % download time). 49 pairs processed altogether with 0 errors so far.\n",
      "Waiting 13.068566373593058 s before next batch.\n",
      "Processed batch 1 in 281.57824571983656 s ( 78.0 % download time). 339 pairs processed altogether with 0 errors so far.\n",
      "Processed batch 2 in 312.96887385760783 s ( 79.8 % download time). 629 pairs processed altogether with 1 errors so far.\n",
      "Processed batch 3 in 424.04416221274005 s ( 86.3 % download time). 919 pairs processed altogether with 5 errors so far.\n",
      "Processed batch 4 in 315.66441633505747 s ( 82.9 % download time). 1209 pairs processed altogether with 5 errors so far.\n",
      "Processed batch 5 in 360.38804481700936 s ( 84.2 % download time). 1499 pairs processed altogether with 6 errors so far.\n",
      "Processed batch 6 in 259.56675474951044 s ( 76.7 % download time). 1789 pairs processed altogether with 8 errors so far.\n",
      "Processed batch 7 in 217.56075184248039 s ( 73.9 % download time). 2079 pairs processed altogether with 8 errors so far.\n",
      "Processed batch 8 in 230.43410768482136 s ( 72.2 % download time). 2369 pairs processed altogether with 8 errors so far.\n",
      "Processed batch 9 in 235.34905891689414 s ( 74.8 % download time). 2659 pairs processed altogether with 9 errors so far.\n",
      "Processed batch 10 in 219.9206537661812 s ( 73.0 % download time). 2949 pairs processed altogether with 9 errors so far.\n",
      "Processed batch 11 in 209.348821690568 s ( 70.8 % download time). 3239 pairs processed altogether with 9 errors so far.\n",
      "Processed batch 12 in 215.7006135457923 s ( 70.9 % download time). 3529 pairs processed altogether with 9 errors so far.\n",
      "Processed batch 13 in 224.30036778614158 s ( 73.3 % download time). 3819 pairs processed altogether with 9 errors so far.\n",
      "Processed batch 14 in 174.91344993641542 s ( 69.4 % download time). 4109 pairs processed altogether with 9 errors so far.\n",
      "Processed batch 15 in 247.58502402130398 s ( 75.2 % download time). 4399 pairs processed altogether with 12 errors so far.\n",
      "Processed batch 16 in 265.593891279801 s ( 77.7 % download time). 4689 pairs processed altogether with 12 errors so far.\n",
      "Processed batch 17 in 247.64350230258424 s ( 77.8 % download time). 4979 pairs processed altogether with 17 errors so far.\n",
      "Processed batch 18 in 216.068418462688 s ( 74.9 % download time). 5269 pairs processed altogether with 17 errors so far.\n",
      "Processed batch 19 in 168.9820146033453 s ( 64.6 % download time). 5559 pairs processed altogether with 17 errors so far.\n",
      "Processed batch 20 in 200.99556096631568 s ( 69.4 % download time). 5849 pairs processed altogether with 17 errors so far.\n",
      "Processed batch 21 in 204.69411001726985 s ( 70.8 % download time). 6139 pairs processed altogether with 17 errors so far.\n",
      "Processed batch 22 in 177.46360965176427 s ( 63.6 % download time). 6429 pairs processed altogether with 26 errors so far.\n",
      "Processed batch 23 in 244.10483355757606 s ( 75.7 % download time). 6719 pairs processed altogether with 26 errors so far.\n",
      "Processed batch 24 in 266.0654994979559 s ( 81.0 % download time). 7009 pairs processed altogether with 28 errors so far.\n",
      "Processed batch 25 in 227.02637706469977 s ( 74.8 % download time). 7299 pairs processed altogether with 30 errors so far.\n",
      "Processed batch 26 in 210.8574582649453 s ( 72.6 % download time). 7589 pairs processed altogether with 30 errors so far.\n",
      "Processed batch 27 in 176.22643110906938 s ( 68.5 % download time). 7879 pairs processed altogether with 30 errors so far.\n",
      "Processed batch 28 in 228.70885161406477 s ( 75.4 % download time). 8169 pairs processed altogether with 31 errors so far.\n",
      "Processed batch 29 in 174.41828861398972 s ( 66.8 % download time). 8459 pairs processed altogether with 31 errors so far.\n",
      "Processed batch 30 in 276.94056598674797 s ( 80.0 % download time). 8749 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 31 in 227.50462363786937 s ( 76.0 % download time). 9039 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 32 in 178.50487849973433 s ( 67.0 % download time). 9329 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 33 in 231.85467071335006 s ( 75.7 % download time). 9619 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 34 in 191.3283565107704 s ( 71.9 % download time). 9909 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 35 in 167.4565110662952 s ( 67.0 % download time). 10199 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 36 in 166.71257839989266 s ( 66.8 % download time). 10489 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 37 in 190.03994845873967 s ( 70.0 % download time). 10779 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 38 in 187.9445436521928 s ( 71.6 % download time). 11069 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 39 in 177.05401194097067 s ( 67.0 % download time). 11359 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 40 in 204.84821042032854 s ( 71.3 % download time). 11649 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 41 in 174.83330441486032 s ( 68.6 % download time). 11939 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 42 in 205.6383766209765 s ( 69.5 % download time). 12229 pairs processed altogether with 43 errors so far.\n",
      "Processed batch 43 in 215.49753586128645 s ( 73.9 % download time). 12519 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 44 in 236.33051032104413 s ( 75.7 % download time). 12809 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 45 in 289.41080546019657 s ( 79.2 % download time). 13099 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 46 in 267.66369270940777 s ( 79.9 % download time). 13389 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 47 in 282.39615886769025 s ( 78.9 % download time). 13679 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 48 in 243.25642288864765 s ( 75.1 % download time). 13969 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 49 in 308.68480243938393 s ( 78.7 % download time). 14259 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 50 in 217.67934623381007 s ( 72.6 % download time). 14549 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 51 in 228.31666358519578 s ( 74.1 % download time). 14839 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 52 in 179.41691076706047 s ( 64.7 % download time). 15129 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 53 in 211.59945898743172 s ( 73.0 % download time). 15419 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 54 in 224.53579358465504 s ( 72.9 % download time). 15709 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 55 in 250.95431885607832 s ( 77.4 % download time). 15999 pairs processed altogether with 44 errors so far.\n",
      "Processed batch 56 in 182.33463191518968 s ( 68.1 % download time). 16289 pairs processed altogether with 44 errors so far.\n",
      "Completed.\n",
      "Starting processing: 44 pairs in 1 batches.\n",
      "Processed batch 0 in 27.80099270970095 s ( 69.3 % download time). 44 pairs processed altogether with 1 errors so far.\n",
      "Waiting 32.19900729029905 s before next batch.\n",
      "Completed.\n",
      "Starting processing: 1 pairs in 1 batches.\n",
      "Processed batch 0 in 0.8632990388578037 s ( 99.9 % download time). 1 pairs processed altogether with 1 errors so far.\n",
      "Waiting 59.136700961142196 s before next batch.\n",
      "Completed.\n",
      "Abort - all pairs processed this batch produced errors.\n"
     ]
    }
   ],
   "source": [
    "# Execute data collection from API and write to DB\n",
    "# Multithreaded version\n",
    "# Auto-looping to reprocess pairs with errors\n",
    "\n",
    "while len(currentpairs) > 0:\n",
    "    # Batching of OD pairs into 290s\n",
    "    startpos = 0\n",
    "    nbatches = len(currentpairs) // 290 + 1\n",
    "    initialbatchsize = len(currentpairs) % 290\n",
    "    processed = set()\n",
    "    errors = []\n",
    "\n",
    "    # timed start\n",
    "    while datetime.datetime.now() < batchsettings['processingstarttime']:\n",
    "        print('Time is', datetime.datetime.now(), '. Starting processing at', batchsettings['processingstarttime'])\n",
    "        tm.sleep(10)\n",
    "        clear_output()\n",
    "\n",
    "    print(\"Starting processing:\", len(currentpairs), \"pairs in\", nbatches, \"batches.\")\n",
    "\n",
    "    for batch in range(nbatches):\n",
    "        outputs = []\n",
    "        starttime = tm.clock()\n",
    "        pool = ThreadPool(12)\n",
    "\n",
    "        # multithreaded query the next batch of 290\n",
    "        outputs.extend(pool.starmap(call_api, [(pair, stoplist, batchsettings) for pair in currentpairs[startpos : (initialbatchsize + batch * 290)]]))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        downloadtime = tm.clock() - starttime\n",
    "\n",
    "        # single-thread write this batch's outputs to db - to avoid schema locks\n",
    "        for output in outputs:\n",
    "            for option in output:\n",
    "                if option[4]:\n",
    "                    write_to_db(db, option[0][0][0], option[0][1][0], option[1], option[2])\n",
    "                    write_to_network(edges, nodes, option[3])\n",
    "                    processed.add(option[0])\n",
    "                else:\n",
    "                    errors.append(option[0])\n",
    "\n",
    "        processingtime = tm.clock() - starttime\n",
    "\n",
    "        print('Processed batch', batch, 'in', processingtime, 's (', str(downloadtime/processingtime * 100)[:4], '% download time).', \n",
    "              len(processed) + len(errors), 'pairs processed altogether with', len(errors), 'errors so far.')\n",
    "\n",
    "        # update next batch's startpos to be this batch's endpos\n",
    "        startpos = initialbatchsize + batch * 290\n",
    "\n",
    "        # check if processing time has expired\n",
    "        if datetime.datetime.now() > batchsettings['processingendtime']:\n",
    "            print('Abort - processing time ended at', datetime.datetime.now())\n",
    "            break\n",
    "\n",
    "        # if finished this batch in less than 60s, wait until 60s has passed for this batch\n",
    "        if processingtime < 60:\n",
    "            print('Waiting', 60 - processingtime, 's before next batch.')\n",
    "            tm.sleep(60 - processingtime)\n",
    "    print('Completed.')\n",
    "    \n",
    "    # check if processing time has expired\n",
    "    if datetime.datetime.now() > batchsettings['processingendtime']:\n",
    "        print('Abort - processing time ended at', datetime.datetime.now())\n",
    "        break\n",
    "        \n",
    "    # if all pairs processed this round produced errors, then break the processing\n",
    "    if (len(errors) == len(currentpairs)):\n",
    "        print('Abort - all pairs processed this batch produced errors.')\n",
    "        break\n",
    "    else:\n",
    "        currentpairs = [pair for pair in currentpairs if pair not in processed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-0156e4be4eea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'20190322_0830/networkdata_900000'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# updated 20190217/1035\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('20190322_0830/networkdata_900000', 'wb') as outfile: # updated 20190217/1035\n",
    "    pickle.dump((edges, nodes), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pickle dump fails due to low memory, dump to csv instead\n",
    "with open('20190322_0830/networkdata_900000_edges.csv', 'w') as outfile:\n",
    "    dictwriter = csv.writer(outfile, delimiter = ';')\n",
    "    for key, val in edges.items():\n",
    "        dictwriter.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentpairs = [pair for pair in currentpairs if pair not in processed] # run 20190218/0048\n",
    "len(currentpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20190322_0830/currentpairs850000', 'wb') as outfile:\n",
    "    pickle.dump(currentpairs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('E02000048', '-0.212724446', '51.60219278'),\n",
       "  ('E02000048', '-0.215295512', '51.60251404'))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentpairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding missing pairs\n",
    "db = sqlite3.connect('20190322_0830/traveloptions.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 66.73182630050388 m/min\n",
      "Mean 4.003909578030228 km/h\n"
     ]
    }
   ],
   "source": [
    "# calculate average walk speed\n",
    "walktrips = pd.read_sql('SELECT orig_id, dest_id, option_id, traveltime, dist FROM traveloptions WHERE legs = 1 AND legs_walking = 1', db)\n",
    "walkspeed = np.mean(walktrips['dist'] / walktrips['traveltime'])\n",
    "print('Mean', walkspeed, 'm/min')\n",
    "print('Mean', np.mean((walktrips['dist'] / 1000) / (walktrips['traveltime'] / 60)), 'km/h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974 9\n",
      "[(('E02000954', '-0.162348358', '51.43267961'), ('E02000954', '-0.16239404', '51.43267485')), (('E02000345', '-0.069908973', '51.57408545'), ('E02000345', '-0.070098658', '51.57381318')), (('E02000444', '-0.301298513', '51.59715682'), ('E02000444', '-0.301204663', '51.59712327')), (('E02000638', '-0.124655411', '51.44946268'), ('E02000638', '-0.124364297', '51.44954231')), (('E02000438', '-0.363354407', '51.60730078'), ('E02000438', '-0.363501163', '51.60634633')), (('E02000648', '-0.100383308', '51.42597135'), ('E02000648', '-0.099983279', '51.42511491')), (('E02000058', '-0.197840395', '51.58261777'), ('E02000058', '-0.198032396', '51.58270095')), (('E02000315', '0.113142073', '51.49371007'), ('E02000315', '0.113230423', '51.49366285')), (('E02000048', '-0.212724446', '51.60219278'), ('E02000048', '-0.215295512', '51.60251404'))]\n"
     ]
    }
   ],
   "source": [
    "pairs_present = 0\n",
    "pairs_missing = []\n",
    "for coord in orig_coords:\n",
    "    if len(pd.read_sql('SELECT orig_id, dest_id, option_id FROM traveloptions WHERE orig_id == \"' + coord[0] + '\" AND dest_id == \"' + coord[0] + '\"', db)) > 0:\n",
    "        pairs_present += 1\n",
    "    else:\n",
    "        pairs_missing.extend([pair for pair in odpairs if pair[0][0] == coord[0] and pair[1][0] == coord[0]])\n",
    "\n",
    "print(pairs_present, len(pairs_missing))\n",
    "print(pairs_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "writecursor = db.cursor()\n",
    "for pair in pairs_missing:\n",
    "    dist = calculate_distance([[pair[0][1], pair[0][2]], [pair[1][1], pair[1][2]]])\n",
    "    walktime = dist / walkspeed\n",
    "    params = (pair[0][0], pair[1][0], 0, '2019-03-22T08:27:00', walktime, 0, 0, 1, dist, None, None, dist, 1, walktime)\n",
    "    writecursor.execute('''INSERT INTO traveloptions(orig_id, dest_id, option_id, starttime, traveltime,\n",
    "    fare, transfers, legs, dist, first_zoned_station, last_zoned_station, dist_walking, legs_walking, time_walking)\n",
    "    VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?)''',params)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
